
@inproceedings{cannici2019AsynchronousConvolutional,
  title = {Asynchronous {{Convolutional Networks}} for {{Object Detection}} in {{Neuromorphic Cameras}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Cannici, M. and Ciccone, M. and Romanoni, A. and Matteucci, M.},
  year = {2019},
  pages = {1656--1665},
  file = {D\:\\ZoteroData\\storage\\YQSB47DQ\\Cannici et al_2019_Asynchronous Convolutional Networks for Object Detection in Neuromorphic Cameras.pdf}
}

@inproceedings{cho2014LearningPhrase,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}\textendash{{Decoder}} for {{Statistical Machine Translation}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Cho, Kyunghyun and {van Merrienboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  pages = {1724--1734},
  publisher = {{Association for Computational Linguistics}},
  address = {{Doha, Qatar}},
  doi = {10.3115/v1/D14-1179},
  language = {en}
}

@inproceedings{czech2016EvaluatingNoise,
  title = {Evaluating Noise Filtering for Event-Based Asynchronous Change Detection Image Sensors},
  booktitle = {2016 6th {{IEEE International Conference}} on {{Biomedical Robotics}} and {{Biomechatronics}} ({{BioRob}})},
  author = {Czech, Daniel and Orchard, Garrick},
  year = {2016},
  month = jun,
  pages = {19--24},
  publisher = {{IEEE}},
  address = {{Singapore, Singapore}},
  doi = {10.1109/BIOROB.2016.7523452},
  abstract = {Bio-inspired Address Event Representation change detection image sensors, also known as silicon retinae, have matured to the point where they can be purchased commercially, and are easily operated by laymen. Noise is present in the output of these sensors, and improved noise filtering will enhance performance in many applications. A novel approach is proposed for quantifying the quality of data received from a silicon retina, and quantifying the performance of different noise filtering algorithms. We present a test rig which repetitively records printed test patterns, along with a method for averaging over repeated recordings to estimate the likelihood of an event being signal or noise. The calculated signal and noise probabilities are used to quantitatively compare the performance of 8 different filtering algorithms while varying each filter's parameters. We show how the choice of best filter and parameters varies as a function of the stimulus, particularly the temporal rate of change of intensity for a pixel, especially when the assumption of sharp temporal edges is not valid.},
  isbn = {978-1-5090-3287-7},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\ZZM6RK3V\\Czech and Orchard - 2016 - Evaluating noise filtering for event-based asynchr.pdf}
}

@article{deng2021DeepConvolutional,
  ids = {deng2020DeepConvolutional},
  title = {Deep {{Convolutional Neural Network}} for {{Multi}}-{{Modal Image Restoration}} and {{Fusion}}},
  author = {Deng, Xin and Dragotti, Pier Luigi},
  year = {2021},
  month = oct,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {10},
  pages = {3333--3348},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2984244}
}

@article{graves2005FramewisePhoneme,
  title = {Framewise Phoneme Classification with Bidirectional {{LSTM}} and Other Neural Network Architectures},
  author = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  year = {2005},
  month = jul,
  journal = {Neural Networks},
  volume = {18},
  number = {5-6},
  pages = {602--610},
  issn = {08936080},
  doi = {10.1016/j.neunet.2005.06.042},
  language = {en},
  annotation = {BiLSTM}
}

@article{hochreiter1997LongShortTerm,
  ids = {sepphochreiter1997LongShortTerm},
  title = {Long {{Short}}-{{Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\FVBDLQFZ\\2604.pdf}
}

@article{hopfield1982NeuralNetworks,
  title = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities.},
  author = {Hopfield, J. J.},
  year = {1982},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {79},
  number = {8},
  pages = {2554--2558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.79.8.2554},
  language = {en}
}

@inproceedings{hu2020DDD20EndtoEnd,
  title = {{{DDD20 End}}-to-{{End Event Camera Driving Dataset}}: {{Fusing Frames}} and {{Events}} with {{Deep Learning}} for {{Improved Steering Prediction}}},
  shorttitle = {{{DDD20 End}}-to-{{End Event Camera Driving Dataset}}},
  booktitle = {2020 {{IEEE}} 23rd {{International Conference}} on {{Intelligent Transportation Systems}} ({{ITSC}})},
  author = {Hu, Yuhuang and Binas, Jonathan and Neil, Daniel and Liu, Shih-Chii and Delbruck, Tobi},
  year = {2020},
  month = sep,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Rhodes, Greece}},
  doi = {10.1109/ITSC45102.2020.9294515},
  abstract = {Neuromorphic event cameras are useful for dynamic vision problems under difficult lighting conditions. To enable studies of using event cameras in automobile driving applications, this paper reports a new end-to-end driving dataset called DDD20. The dataset was captured with a DAVIS camera that concurrently streams both dynamic vision sensor (DVS) brightness change events and active pixel sensor (APS) intensity frames. DDD20 is the longest event camera end-toend driving dataset to date with 51h of DAVIS event+frame camera and vehicle human control data collected from 4000 km of highway and urban driving under a variety of lighting conditions. Using DDD20, we report the first study of fusing brightness change events and intensity frame data using a deep learning approach to predict the instantaneous human steering wheel angle. Over all day and night conditions, the explained variance for human steering prediction from a Resnet-32 is significantly better from the fused DVS+APS frames (0.88) than using either DVS (0.67) or APS (0.77) data alone.},
  isbn = {978-1-72814-149-7},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\LTD92K8Z\\Hu et al. - 2020 - DDD20 End-to-End Event Camera Driving Dataset Fus.pdf}
}

@article{huang2018EventGuidedStructured,
  title = {Event-{{Guided Structured Output Tracking}} of {{Fast}}-{{Moving Objects Using}} a {{CeleX Sensor}}},
  author = {Huang, Jing and Wang, Shizheng and Guo, Menghan and Chen, Shoushun},
  year = {2018},
  month = sep,
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume = {28},
  number = {9},
  pages = {2413--2417},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2018.2841516},
  abstract = {In this paper, we propose an event-guided support vector machine (ESVM) for tracking high-speed moving objects. Tracking fastmoving objects with low frame rate cameras is always difficult due to motion blur and large displacements. The accuracy problem can be solved by using high frame rate cameras at the expense of tremendous computational cost. For this issue, our ESVM incorporates event-based guiding methods into the traditional structured support vector machine to improve the tracking accuracy at a relatively low-complexity level. The event-based guiding methods include two models, event position guided search localization and event intensity guided sample supplement, which are based on the event features of the CeleX motion sensor. The motion sensor continuously responds to intensity change, which is generally related to object motion. Once it has detected intensity change, the motion sensor outputs event packages, and each of them contains the pixel location, time stamp, and pixel illumination. The generated events are continuous in the temporal domain and thus record the motion trajectory of fast-moving objects, which cannot be fully captured by frame-based cameras. In this paper, we convert high-speed test sequences into sequences of spiking events recorded by the CeleX motion sensor. Our approach presents fairly high computational efficiency, and experiments over sequences from multiple tracking benchmarks demonstrate the superior accuracy and real-time performance of our method, compared to the state-of-the-art trackers.},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\CVXF2VBQ\\Huang et al. - 2018 - Event-Guided Structured Output Tracking of Fast-Mo.pdf}
}

@article{jiang2020ObjectTracking,
  ids = {jiang2020object},
  title = {Object Tracking on Event Cameras with Offline\textendash Online Learning},
  author = {Jiang, Rui and Mou, Xiaozheng and Shi, Shunshun and Zhou, Yueyin and Wang, Qinyi and Dong, Meng and Chen, Shoushun},
  year = {2020},
  journal = {CAAI Transactions on Intelligence Technology},
  volume = {5},
  number = {3},
  pages = {165--171},
  publisher = {{IET}}
}

@article{krizhevsky2017ImageNetClassification,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {00010782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  language = {en},
  annotation = {AlexNet},
  file = {D\:\\ZoteroData\\storage\\6WP9KZQT\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@inproceedings{krull2019Noise2voidLearning,
  title = {Noise2void - Learning Denoising from Single Noisy Images},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF}} Conference on Computer Vision and Pattern Recognition},
  author = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
  year = {2019},
  pages = {2129--2137},
  keywords = {‚ñ∂ 5-ÂæÖËØª},
  file = {D\:\\ZoteroData\\storage\\HMVDIH89\\Krull et al. - 2019 - Noise2Void - Learning Denoising from Single Noisy .pdf}
}

@article{lecun1998GradientbasedLearning,
  ids = {yannlecun1998GradientBasedLearning},
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {Nov./1998},
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {00189219},
  doi = {10.1109/5.726791},
  file = {D\:\\ZoteroData\\storage\\SHHA7WKU\\1998Lecun.pdf}
}

@inproceedings{lehtinen2018Noise2NoiseLearning,
  title = {{{Noise2Noise}}: {{Learning}} Image Restoration without Clean Data},
  booktitle = {{{ICML}}},
  author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
  year = {2018},
  pages = {2971--2980},
  keywords = {‚úî 7-Á≤æËØª,üôÇ 2 - ! !},
  file = {D\:\\ZoteroData\\storage\\ITXI4S3J\\Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Cl.pdf}
}

@article{lenero-bardallo2011UsLatency,
  title = {A 3.6 Us {{Latency Asynchronous Frame}}-{{Free Event}}-{{Driven Dynamic}}-{{Vision}}-{{Sensor}}},
  author = {{Lenero-Bardallo}, Juan Antonio and {Serrano-Gotarredona}, Teresa and {Linares-Barranco}, Bernab{\'e}},
  year = {2011},
  month = jun,
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {46},
  number = {6},
  pages = {1443--1455},
  issn = {0018-9200, 1558-173X},
  doi = {10.1109/JSSC.2011.2118490}
}

@inproceedings{li2014MedicalImage,
  title = {Medical Image Classification with Convolutional Neural Network},
  booktitle = {2014 13th {{International Conference}} on {{Control Automation Robotics}} \& {{Vision}} ({{ICARCV}})},
  author = {Li, Qing and Cai, Weidong and Wang, Xiaogang and Zhou, Yun and Feng, David Dagan and Chen, Mei},
  year = {2014},
  month = dec,
  pages = {844--848},
  publisher = {{IEEE}},
  address = {{Singapore}},
  doi = {10.1109/ICARCV.2014.7064414},
  isbn = {978-1-4799-5199-4}
}

@article{lichtsteiner2008128128,
  ids = {lichtsteiner2008128Times},
  title = {A 128\texttimes 128 120 {{dB}} 15us {{Latency Asynchronous Temporal Contrast Vision Sensor}}},
  author = {Lichtsteiner, Patrick and Posch, Christoph and Delbruck, Tobi},
  year = {2008},
  journal = {IEEE Journal of Solid-State Circuits},
  volume = {43},
  number = {2},
  pages = {566--576},
  issn = {0018-9200},
  doi = {10.1109/JSSC.2007.914337},
  abstract = {This paper describes a 128 128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1\% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 s at 1 klux pixel illumination. The sensor is built in a 0.35 m 4M2P process. It has 40 40 m2 pixels with 9.4\% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\JGW88SX3\\Lichtsteiner et al. - 2008 - A 128$times$128 120 dB 15 $mu$s Latency Asynchro.pdf}
}

@inproceedings{liu2015DesignSpatiotemporal,
  title = {Design of a Spatiotemporal Correlation Filter for Event-Based Sensors},
  booktitle = {2015 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Liu, Hongjie and Brandli, Christian and Li, Chenghan and Liu, Shih-Chii and Delbruck, Tobi},
  year = {2015},
  month = may,
  pages = {722--725},
  publisher = {{IEEE}},
  address = {{Lisbon, Portugal}},
  doi = {10.1109/ISCAS.2015.7168735},
  abstract = {This paper reports the design of a 1mW, 10nslatency mixed signal system in 0.18{$\mu$}m CMOS which enables filtering out uncorrelated background activity in event-based neuromorphic sensors. Background activity (BA) in the output of dynamic vision sensors is caused by thermal noise and junction leakage current acting on switches connected to floating nodes in the pixels. The reported chip generates a pass flag for spatiotemporally correlated events for post-processing to reduce communication/computation load and improve information rate. A chip with 128\texttimes 128 array with 20\texttimes 20{$\mu$}m2 cells has been designed. Each filter cell combines programmable spatial subsampling with a temporal window based on current integration. Power-gating is used to minimize the power consumption by only activating the threshold detection and communication circuits in the cell receiving an input event. This correlation filter chip targets embedded neuromorphic visual and auditory systems, where low average power consumption and low latency are critical.},
  isbn = {978-1-4799-8391-9},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\KLI9MD9T\\Liu et al. - 2015 - Design of a spatiotemporal correlation filter for .pdf;D\:\\ZoteroData\\storage\\T7BQPQZM\\Liu et al. - 2015 - Design of a spatiotemporal correlation filter for .pdf}
}

@inproceedings{maqueda2018event,
  title = {Event-Based Vision Meets Deep Learning on Steering Prediction for Self-Driving Cars},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Maqueda, Ana I and Loquercio, Antonio and Gallego, Guillermo and Garc{\'i}a, Narciso and Scaramuzza, Davide},
  year = {2018},
  pages = {5419--5427}
}

@inproceedings{moeys2016SteeringPredator,
  title = {Steering a Predator Robot Using a Mixed Frame/Event-Driven Convolutional Neural Network},
  booktitle = {2016 {{Second International Conference}} on {{Event}}-Based {{Control}}, {{Communication}}, and {{Signal Processing}} ({{EBCCSP}})},
  author = {Moeys, Diederik Paul and Corradi, Federico and Kerr, Emmett and Vance, Philip and Das, Gautham and Neil, Daniel and Kerr, Dermot and Delbruck, Tobi},
  year = {2016},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Krakow, Poland}},
  doi = {10.1109/EBCCSP.2016.7605233},
  abstract = {This paper describes the application of a Convolutional Neural Network (CNN) in the context of a predator/prey scenario. The CNN is trained and run on data from a Dynamic and Active Pixel Sensor (DAVIS) mounted on a Summit XL robot (the predator), which follows another one (the prey). The CNN is driven by both conventional image frames and dynamic vision sensor "frames" that consist of a constant number of DAVIS ON and OFF events. The network is thus "data driven" at a sample rate proportional to the scene activity, so the effective sample rate varies from 15 Hz to 240 Hz depending on the robot speeds. The network generates four outputs: steer right, left, center and non-visible. After off-line training on labeled data, the network is imported on the on-board Summit XL robot which runs jAER and receives steering directions in real time. Successful results on closed-loop trials, with accuracies up to 87\% or 92\% (depending on evaluation criteria) are reported. Although the proposed approach discards the precise DAVIS event timing, it offers the significant advantage of compatibility with conventional deep learning technology without giving up the advantage of datadriven computing.},
  isbn = {978-1-5090-4196-1},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\SQI7EDR5\\Moeys et al. - 2016 - Steering a predator robot using a mixed frameeven.pdf}
}

@article{mueggler2017EventCameraDataset,
  title = {The {{Event}}-{{Camera Dataset}} and {{Simulator}}: {{Event}}-Based {{Data}} for {{Pose Estimation}}, {{Visual Odometry}}, and {{SLAM}}},
  shorttitle = {The {{Event}}-{{Camera Dataset}} and {{Simulator}}},
  author = {Mueggler, Elias and Rebecq, Henri and Gallego, Guillermo and Delbruck, Tobi and Scaramuzza, Davide},
  year = {2017},
  month = feb,
  journal = {The International Journal of Robotics Research},
  volume = {36},
  number = {2},
  eprint = {1610.08336},
  eprinttype = {arxiv},
  pages = {142--149},
  issn = {0278-3649, 1741-3176},
  doi = {10.1177/0278364917691115},
  abstract = {New vision sensors, such as the Dynamic and Active-pixel Vision sensor (DAVIS), incorporate a conventional globalshutter camera and an event-based sensor in the same pixel array. These sensors have great potential for high-speed robotics and computer vision because they allow us to combine the benefits of conventional cameras with those of event-based sensors: low latency, high temporal resolution, and very high dynamic range. However, new algorithms are required to exploit the sensor characteristics and cope with its unconventional output, which consists of a stream of asynchronous brightness changes (called ``events'') and synchronous grayscale frames. For this purpose, we present and release a collection of datasets captured with a DAVIS in a variety of synthetic and real environments, which we hope will motivate research on new algorithms for high-speed and high-dynamic-range robotics and computer-vision applications. In addition to global-shutter intensity images and asynchronous events, we provide inertial measurements and ground-truth camera poses from a motion-capture system. The latter allows comparing the pose accuracy of egomotion estimation algorithms quantitatively. All the data are released both as standard text files and binary files (i.e., rosbag). This paper provides an overview of the available data and describes a simulator that we release open-source to create synthetic event-camera data.},
  archiveprefix = {arXiv},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\QJQA9G52\\Mueggler et al. - 2017 - The Event-Camera Dataset and Simulator Event-base.pdf}
}

@article{orchard2015ConvertingStatic,
  title = {Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades},
  author = {Orchard, Garrick and Jayawant, Ajinkya and Cohen, Gregory K. and Thakor, Nitish},
  year = {2015},
  journal = {Frontiers in Neuroscience},
  volume = {9},
  pages = {437},
  issn = {1662-453X},
  doi = {10.3389/fnins.2015.00437},
  abstract = {Creating datasets for Neuromorphic Vision is a challenging task. A lack of available recordings from Neuromorphic Vision sensors means that data must typically be recorded specifically for dataset creation rather than collecting and labeling existing data. The task is further complicated by a desire to simultaneously provide traditional frame-based recordings to allow for direct comparison with traditional Computer Vision algorithms. Here we propose a method for converting existing Computer Vision static image datasets into Neuromorphic Vision datasets using an actuated pan-tilt camera platform. Moving the sensor rather than the scene or image is a more biologically realistic approach to sensing and eliminates timing artifacts introduced by monitor updates when simulating motion on a computer monitor. We present conversion of two popular image datasets (MNIST and Caltech101) which have played important roles in the development of Computer Vision, and we provide performance metrics on these datasets using spike-based recognition algorithms. This work contributes datasets for future use in the field, as well as results from spike-based algorithms against which future works can compare. Furthermore, by converting datasets already popular in Computer Vision, we enable more direct comparison with frame-based approaches.},
  file = {D\:\\ZoteroData\\storage\\EY8NQPZM\\Orchard et al. - 2015 - Converting Static Image Datasets to Spiking Neurom.pdf}
}

@article{padala2018NoiseFilteringa,
  title = {A {{Noise Filtering Algorithm}} for {{Event}}-{{Based Asynchronous Change Detection Image Sensors}} on {{TrueNorth}} and {{Its Implementation}} on {{TrueNorth}}},
  author = {Padala, Vandana and Basu, Arindam and Orchard, Garrick},
  year = {2018},
  month = mar,
  journal = {Frontiers in Neuroscience},
  volume = {12},
  pages = {118},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00118},
  file = {D\:\\ZoteroData\\storage\\QQQXCMPJ\\Padala et al. - 2018 - A Noise Filtering Algorithm for Event-Based Asynch.pdf}
}

@article{pal2021DeepLearninga,
  ids = {pal2021DeepLearning},
  title = {Deep Learning in Multi-Object Detection and Tracking: State of the Art},
  shorttitle = {Deep Learning in Multi-Object Detection and Tracking},
  author = {Pal, Sankar K. and Pramanik, Anima and Maiti, J. and Mitra, Pabitra},
  year = {2021},
  month = sep,
  journal = {Applied Intelligence},
  volume = {51},
  number = {9},
  pages = {6400--6429},
  publisher = {{Springer}},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-021-02293-7},
  language = {en},
  keywords = {‚ñ∂ 5-ÂæÖËØª},
  file = {D\:\\ZoteroData\\storage\\GCV22YBH\\Pal et al. - 2021 - Deep learning in multi-object detection and tracki.pdf;D\:\\ZoteroData\\storage\\M2LIFQ2D\\Pal et al_2021_Deep learning in multi-object detection and tracking.pdf;D\:\\ZoteroData\\storage\\E8AV3CA9\\s10489-021-02293-7.html}
}

@article{rebecq2016EvoGeometric,
  title = {Evo: {{A}} Geometric Approach to Event-Based 6-Dof Parallel Tracking and Mapping in Real Time},
  author = {Rebecq, Henri and Horstsch{\"a}fer, Timo and Gallego, Guillermo and Scaramuzza, Davide},
  year = {2016},
  journal = {IEEE Robotics and Automation Letters},
  volume = {2},
  number = {2},
  pages = {593--600},
  publisher = {{IEEE}}
}

@inproceedings{renner2019EventbasedAttention,
  title = {Event-Based Attention and Tracking on Neuromorphic Hardware},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Renner, Alpha and Evanusa, Matthew and Sandamirskaya, Yulia},
  year = {2019},
  pages = {1709--1716},
  publisher = {{IEEE}}
}

@article{taherkhani2020ReviewLearning,
  title = {A Review of Learning in Biologically Plausible Spiking Neural Networks},
  author = {Taherkhani, Aboozar and Belatreche, Ammar and Li, Yuhua and Cosma, Georgina and Maguire, Liam P. and McGinnity, T.M.},
  year = {2020},
  month = feb,
  journal = {Neural Networks},
  volume = {122},
  pages = {253--272},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.09.036},
  language = {en},
  file = {D\:\\ZoteroData\\storage\\WGCULG4V\\Taherkhani et al_2020_A review of learning in biologically plausible spiking neural networks.pdf}
}

@article{vidal2018UltimateSLAM,
  title = {Ultimate {{SLAM}}? {{Combining Events}}, {{Images}}, and {{IMU}} for {{Robust Visual SLAM}} in {{HDR}} and {{High}}-{{Speed Scenarios}}},
  shorttitle = {Ultimate {{SLAM}}?},
  author = {Vidal, Antoni Rosinol and Rebecq, Henri and Horstschaefer, Timo and Scaramuzza, Davide},
  year = {2018},
  month = apr,
  journal = {IEEE Robotics and Automation Letters},
  volume = {3},
  number = {2},
  pages = {994--1001},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2018.2793357}
}

@inproceedings{wang2020StereoEventbased,
  ids = {wang2020stereo},
  title = {Stereo Event-Based Particle Tracking Velocimetry for {{3D}} Fluid Flow Reconstruction},
  booktitle = {Computer Vision \textendash{} {{ECCV}} 2020},
  author = {Wang, Yuanhao and Idoughi, Ramzi and Heidrich, Wolfgang},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  pages = {36--53},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  abstract = {Existing Particle Imaging Velocimetry techniques require the use of high-speed cameras to reconstruct time-resolved fluid flows. These cameras provide high-resolution images at high frame rates, which generates bandwidth and memory issues. By capturing only changes in the brightness with a very low latency and at low data rate, event-based cameras have the ability to tackle such issues. In this paper, we present a new framework that retrieves dense 3D measurements of the fluid velocity field using a pair of event-based cameras. First, we track particles inside the two event sequences in order to estimate their 2D velocity in the two sequences of images. A stereo-matching step is then performed to retrieve their 3D positions. These intermediate outputs are incorporated into an optimization framework that also includes physically plausible regularizers, in order to retrieve the 3D velocity field. Extensive experiments on both simulated and real data demonstrate the efficacy of our approach.},
  isbn = {978-3-030-58526-6},
  organization = {{Springer}}
}

@article{wu2020CoarsetofineClassification,
  title = {Coarse-to-Fine Classification for Diabetic Retinopathy Grading Using Convolutional Neural Network},
  author = {Wu, Zhan and Shi, Gonglei and Chen, Yang and Shi, Fei and Chen, Xinjian and Coatrieux, Gouenou and Yang, Jian and Luo, Limin and Li, Shuo},
  year = {2020},
  month = aug,
  journal = {Artificial Intelligence in Medicine},
  volume = {108},
  pages = {101936},
  issn = {09333657},
  doi = {10.1016/j.artmed.2020.101936},
  language = {en}
}

@inproceedings{xie2017ImprovedApproach,
  title = {An {{Improved Approach}} for {{Visualizing Dynamic Vision Sensor}} and Its {{Video Denoising}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Video}} and {{Image Processing}}  - {{ICVIP}} 2017},
  author = {Xie, Xuemei and Du, Jiang and Shi, Guangming and Hu, Hong and Li, Wang},
  year = {2017},
  pages = {176--180},
  publisher = {{ACM Press}},
  address = {{Singapore, Singapore}},
  doi = {10.1145/3177404.3177411},
  isbn = {978-1-4503-5383-0},
  language = {en},
  keywords = {denoising},
  file = {D\:\\ZoteroData\\storage\\RMNTA6NT\\Xie et al_2017_An Improved Approach for Visualizing Dynamic Vision Sensor and its Video.pdf}
}

@inproceedings{zhang2017LearningDeepa,
  ids = {zhang2017LearningDeep},
  title = {Learning {{Deep CNN Denoiser Prior}} for {{Image Restoration}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhang, Kai and Zuo, Wangmeng and Gu, Shuhang and Zhang, Lei},
  year = {2017},
  month = jul,
  pages = {2808--2817},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.300},
  isbn = {978-1-5386-0457-1}
}

@article{zhu2020ReviewVideo,
  title = {A {{Review}} of {{Video Object Detection}}: {{Datasets}}, {{Metrics}} and {{Methods}}},
  shorttitle = {A {{Review}} of {{Video Object Detection}}},
  author = {Zhu, Haidi and Wei, Haoran and Li, Baoqing and Yuan, Xiaobing and Kehtarnavaz, Nasser},
  year = {2020},
  month = nov,
  journal = {Applied Sciences},
  volume = {10},
  number = {21},
  pages = {7834},
  issn = {2076-3417},
  doi = {10.3390/app10217834},
  abstract = {Although there are well established object detection methods based on static images, their application to video data on a frame by frame basis faces two shortcomings: (i) lack of computational efficiency due to redundancy across image frames or by not using a temporal and spatial correlation of features across image frames, and (ii) lack of robustness to real-world conditions such as motion blur and occlusion. Since the introduction of the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2015, a growing number of methods have appeared in the literature on video object detection, many of which have utilized deep learning models. The aim of this paper is to provide a review of these papers on video object detection. An overview of the existing datasets for video object detection together with commonly used evaluation metrics is first presented. Video object detection methods are then categorized and a description of each of them is stated. Two comparison tables are provided to see their differences in terms of both accuracy and computational efficiency. Finally, some future trends in video object detection to address the challenges involved are noted.},
  language = {en},
  keywords = {‚ñ∂ 5-ÂæÖËØª,‚ôè 8-mark},
  file = {D\:\\ZoteroData\\storage\\2MXEXKB6\\Zhu et al. - 2020 - A Review of Video Object Detection Datasets, Metr.pdf}
}


