[1]	Liu H, Brandli C, Li C, et al. Design of a spatiotemporal correlation filter for event-based sensors[C]//2015 IEEE International Symposium on Circuits and Systems (ISCAS). Lisbon, Portugal: IEEE, 2015: 722–725.
[2]	Czech D, Orchard G. Evaluating noise filtering for event-based asynchronous change detection image sensors[C]//2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob). Singapore, Singapore: IEEE, 2016: 19–24.
[3]	Xie X, Du J, Shi G, et al. An Improved Approach for Visualizing Dynamic Vision Sensor and its Video Denoising[C]//Proceedings of the International Conference on Video and Image Processing  - ICVIP 2017. Singapore, Singapore: ACM Press, 2017: 176–180.
[4]	Padala V, Basu A, Orchard G. A Noise Filtering Algorithm for Event-Based Asynchronous Change Detection Image Sensors on TrueNorth and Its Implementation on TrueNorth[J]. Frontiers in Neuroscience, 2018, 12: 14.
[5]	Mahowald Misha, Douglas Rodney. A silicon neuron[J]. Nature, 1991, 354(6354): 515–518.
[6]	Kleinfelder S, Lim S, Liu X Q, et al. A 128×128 120dB 30mW asynchronous vision sensor that responds to relative intensity change[C]//2006: 10.
[7]	Lichtsteiner P, Posch C, Delbruck T. A 128×128 120 dB 15us Latency Asynchronous Temporal Contrast Vision Sensor[J]. IEEE Journal of Solid-State Circuits, 2008, 43(2): 566–576.
[8]	Posch C, Matolin D, Wohlgenannt R. A QVGA 143 dB Dynamic Range Frame-Free PWM Image Sensor With Lossless Pixel-Level Video Compression and Time-Domain CDS[J]. IEEE Journal of Solid-State Circuits, 2011, 46(1): 259–275.
[9]	Berner Raphael, Brandli Christian, Yang Minhao, et al. A 240x180 120dB 10mW 12us‐latency sparse output vision sensor for mobile applications[R]. Intl. Image Sensors Workshop, Snowbird Resort, UT, USA: 2013.
[10]	Guo M, Ding R, Chen S. Live demonstration: A dynamic vision sensor with direct logarithmic output and full-frame picture-on-demand[C]//2016 IEEE International Symposium on Circuits and Systems (ISCAS). Montréal, QC, Canada: IEEE, 2016: 456–456.
[11]	Schmidhuber J. Deep learning in neural networks: An overview[J]. Neural Networks, 2015, 61: 85–117.
[12]	McCulloch W S, Pitts W. A logical calculus of the ideas immanent in nervous activity[J]. The Bulletin of Mathematical Biophysics, 1943, 5(4): 115–133.
[13]	Rosenblatt F. The perceptron: A probabilistic model for information storage and organization in the brain.[J]. Psychological Review, 1958, 65(6): 386–408.
[14]	Minsky M, Papert S A. Perceptrons: An Introduction to Computational Geometry[M]. MIT Press, 2017.
[15]	Hopfield J J. Neural networks and physical systems with emergent collective computational abilities.[J]. Proceedings of the National Academy of Sciences, 1982, 79(8): 2554–2558.
[16]	Hinton G E, Sejnowski T J. Learning and relearning in Boltzmann machines, Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations[M]. MIT Press, Cambridge, MA, 1986.
[17]	Rumelhart D E, Hinton G E, Williams R J. Learning internal representations by error propagation[R]. California Univ San Diego La Jolla Inst for Cognitive Science, 1985.
[18]	Hornik K, Stinchcombe M, White H. Multilayer feedforward networks are universal approximators[J]. Neural Networks, 1989, 2(5): 359–366.
[19]	Hinton G E, Osindero S, Teh Y-W. A Fast Learning Algorithm for Deep Belief Nets[J]. Neural Computation, 2006, 18(7): 1527–1554.
[20]	Glorot X, Bordes A, Bengio Y. Deep Sparse Rectifier Neural Networks[M]. 2010, 15.
[21]	Tan C, Lallee S, Orchard G. Benchmarking neuromorphic vision: lessons learnt from computer vision[J]. Frontiers in Neuroscience, 2015, 9: 374.
[22]	Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278–2324.
[23]	Orchard G, Jayawant A, Cohen G K, et al. Converting Static Image Datasets to Spiking Neuromorphic Datasets Using Saccades[J]. Frontiers in Neuroscience, 2015, 9.
[24]	Li H M, Liu H C, Ji X Y, et al. CIFAR10-DVS: An Event-Stream Dataset for Object Classification[J]. Frontiers In Neuroscience, 2017, 11.
[25]	Vincent P, Larochelle H, Bengio Y, et al. Extracting and composing robust features with denoising autoencoders[C]//Proceedings of the 25th international conference on Machine learning - ICML ’08. Helsinki, Finland: ACM Press, 2008: 1096–1103.
[26]	Sakib S, Ahmed, Jawad A, et al. An Overview of Convolutional Neural Network: Its Architecture and Applications[M]. 2018.
[27]	Masci J, Meier U, Cireşan D, et al. Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction[G]//Honkela T, Duch W, Girolami M, et al. Artificial Neural Networks and Machine Learning – ICANN 2011. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, 6791: 52–59.
[28]	Kaiming He, Xiangyu Zhang, Shaoqing Ren, et al. Deep Residual Learning for Image Recognition[C]//2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Las Vegas, NV, USA: IEEE, 2016: 770–778.
[29]	Bengio Y, Simard P, Frasconi P. Learning long-term dependencies with gradient descent is difficult[J]. IEEE Transactions on Neural Networks, 1994, 5(2): 157–166.
[30]	Sepp Hochreiter, J urgen Schmidhuber. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8): 1735–1780.
[31]	Cho K, van Merrienboer B, Gulcehre C, et al. Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation[C]//Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics, 2014: 1724–1734.
[32]	Christopher Olah. Understanding LSTM Networks[EB/OL]. colah’s blog, 2015-08-27. (2015-08-27). http://colah.github.io/posts/2015-08-Understanding-LSTMs/.
